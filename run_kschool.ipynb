{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning using AlphaZero methodology\n",
    "\n",
    "Adapted from https://applied-data.science/blog/how-to-build-your-own-alphazero-ai-using-python-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game will control all the mechanisms to play a game, and agent will emulate a player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to display the board, we need to create a logger. Here we just print the board to the standard output, to get a graps of the current situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogger:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def info(log):\n",
    "        # log is a list of chars resembling the board\n",
    "        print(str(log) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing a first game by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play a game by hand\n",
    "game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do in the game?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 36, 37, 38, 39, 40, 41]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.allowedActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the board looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we introduce a token by the top, it will fall to the bottom. At the bottom, we have the positions 35 to 41, so those are the only actions we can do now.\n",
    "\n",
    "For instance, let's put a token right in the middle, it will fall to the middle position at the bottom, that's position 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(38)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two players in this game, 1 and -1. The first player was 1, so the current player should be -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what this player can do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 35, 36, 37, 39, 40, 41]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.allowedActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because position 38 is taken, now the player -1 could put a token on top of it, that's it, position 31. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(31)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who's the next player?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the game going?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the count of games won by each one of the players. Let's make player -1 win the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 35, 36, 37, 39, 40, 41]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.allowedActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(35)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(24)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0x7fc5a680ecf8>, 0, 0, None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(36)\n",
    "game.step(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second element of the tuple is the value. The value 0 means that nothing has happened yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If player 1 moves to position 37, then player 1 will win. But player 1 is dumb, so the next moves are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0x7fc5a680e860>, 0, 0, None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0x7fc5a680e898>, -1, 1, None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', 'X', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the score of the game, we have to check who is the current player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then get the first value of these tuple. The winner of the game is the multiplication of both values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the winner is 0\n"
     ]
    }
   ],
   "source": [
    "print(\"And the winner is %d\" % (game.currentPlayer*game.gameState.score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep playing. We need to clear the board to keep playing, because the game goal is to be the first to make a 4-connect. Once that's done, newer 4-connect will not contribute towards the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<game.GameState at 0x7fc5a680e1d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0x7fc5a68c5630>, 0, 0, None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(38)\n",
    "game.step(31)\n",
    "game.step(35)\n",
    "game.step(24)\n",
    "game.step(36)\n",
    "game.step(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now player 1 has learnt, and will do the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0x7fc5a680eef0>, -1, 1, None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', 'X', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the winner is 1\n"
     ]
    }
   ],
   "source": [
    "print(\"And the winner is %d\" % (game.currentPlayer*game.gameState.score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect that a game has finished, we can monitor the score, or the value returned by each step. When it is different to 0, that means that there has been a winning move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game with an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network using the results of our games, we need to use an agent. The agent needs to use an untrained neural network as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network, we can use any Keras model. Here, we use a function from the game, that needs some configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Residual_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_CNN_LAYERS = [\n",
    "\t{'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_NN = Residual_CNN(config.REG_CONST, config.LEARNING_RATE, (2,) + game.grid_shape, game.action_size, HIDDEN_CNN_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 3   # number of simulations the agent will attempt to search for the best next movement\n",
    "CPUCT = 1  # constant controlling the level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"Lee Sedol del Conecta4\", game.state_size, game.action_size, NUM_OF_SIMULATIONS, CPUCT, current_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a blank state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agent will decide what to do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_action, probs, MCTS_value, NN_value = agent.act(state, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the positions in the board, `next_action` is the position with the maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0.5, 0. ,\n",
       "       0. , 0. , 0. ])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a vector with the probability of all the positions in the board. For instance, we can check that all positions with prob > 0 are in fact allowed actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 36, 37, 38, 39, 40, 41]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.allowedActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35],\n",
       "       [37]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(probs > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, value, _, _ = game.step(next_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `act` method, the second argument should be 0 for a random movement, and 1 for a calculated movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is the turn of the second player (who plays randomly)\n",
    "next_action, probs, _, _ = agent.act(state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.33333333])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 36, 37, 38, 39, 40, 41]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.allowedActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, value, _, _ = game.step(next_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep playing with this agent, that will try to find the best movements for the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_action, probs, _, _ = agent.act(state, 1)\n",
    "state, value, _, _ = game.step(next_action)\n",
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', 'O', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_action, probs, _, _ = agent.act(state, 0)\n",
    "state, value, _, _ = game.step(next_action)\n",
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', 'O', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_action, probs, _, _ = agent.act(state, 1)\n",
    "state, value, _, _ = game.step(next_action)\n",
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: a learning agent against a random player\n",
    "\n",
    "Now that you know how to run a learning agent in a game, write a function that given an agent returns the outcome of the game.\n",
    "\n",
    "Don't worry about keeping the memory of the positions. We just want the final outcome of the game, from the learning agent point of view: WIN, DRAW or LOSS.\n",
    "\n",
    "The game will be randomly started either by the random player or the neural network.\n",
    "\n",
    "We will later use this function to run several simulations.\n",
    "\n",
    "Use this logger to keep track of:\n",
    "* each new action suggested by the agent (both for the NN and for the random player)\n",
    "* value after each movement\n",
    "* a render of the board (you can use state.render(logger))\n",
    "* if the movement is done by the NN, the values of the MonteCarlo tree search, and the NN network\n",
    "* a big WARNING if the agent suggest a movement that is not allowed by the state of the board\n",
    "\n",
    "The function will return a tuple, with the result of the game, and the number of movements of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_logger\n",
    "\n",
    "logger_simgame = setup_logger('logger_simgame', 'logs/logger_simgame.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student version cell\n",
    "def simgame(game, agent, logger):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @return a tuple with the result of the game and the number of movements of the NN\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simgame(game, agent, logger):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @return a tuple with the result of the game and the number of movements of the NN\n",
    "    \"\"\"\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    logger.info(\"NEW GAME\")\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    \n",
    "    state = game.reset()\n",
    "    \n",
    "    # 0 -> the neural network starts\n",
    "    # 1 -> the random player starts\n",
    "    who_starts = random.choice([0,1])\n",
    "    \n",
    "    # Tau is the parameter that controls the act method, 0 is random, 1 is neural network\n",
    "    if who_starts == 0:\n",
    "        tau = 1  # NN starts\n",
    "        logger.info(\"Game started by neural network. NN will be the X\")\n",
    "        nn_symbol, rnd_symbol = \"X\", \"O\"\n",
    "    else:\n",
    "        tau = 0  # Random player starts\n",
    "        logger.info(\"Game started by random player. NN will be the O\")\n",
    "        nn_symbol, rnd_symbol = \"O\", \"X\"\n",
    "        \n",
    "    game_is_ended = False\n",
    "    winner = -2  # we init with an impossible value\n",
    "    nn_movements = 0\n",
    "    while not game_is_ended:\n",
    "        next_action, _, MCTS_value, NN_value = agent.act(state, tau)\n",
    "        state, score, _, _ = game.step(next_action)\n",
    "        state.render(logger)\n",
    "        if tau == 1:\n",
    "            logger.info(\"NN (%s) played, moved to %d\" % (nn_symbol, next_action))\n",
    "            tau = 0\n",
    "            nn_movements += 1\n",
    "        else:\n",
    "            tau = 1\n",
    "            logger.info(\"Random (%s) played, moved to %d\" % (rnd_symbol, next_action))\n",
    "            \n",
    "        logger.info(\"Game score: %d     MCTS: %.4f          NN: %.4f\" % (score, MCTS_value, NN_value))\n",
    "        if state.isEndGame != 0:\n",
    "            game_is_ended = True\n",
    "            winner = game.currentPlayer*score\n",
    "            # If random started, then the result of the game is the opposite\n",
    "            if who_starts == 1:\n",
    "                winner = winner*(-1)\n",
    "            if winner == 1:\n",
    "                logger.info(\" **** The NN has WON! :D ****\")\n",
    "            elif winner == 0:\n",
    "                logger.info(\" **** It is a DRAW :S ****\")\n",
    "            else:\n",
    "                logger.info(\" **** The NN has LOST :'( ****\")\n",
    "                \n",
    "    return winner, nn_movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the agent learnt?\n",
    "\n",
    "Let's try a new agent each time, and plot some stats about the number of wins, and the distribution of the number of movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 15   # number of simulations the agent will attempt to search for the best next movement\n",
    "CPUCT = 1  # constant controlling the level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_CNN_LAYERS = [\n",
    "\t{'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t]\n",
    "\n",
    "game = Game()\n",
    "current_NN = Residual_CNN(config.REG_CONST, config.LEARNING_RATE, (2,) + game.grid_shape, game.action_size, HIDDEN_CNN_LAYERS)\n",
    "agent = Agent(\"Lee Sedol del Conecta4\", game.state_size, game.action_size, NUM_OF_SIMULATIONS, CPUCT, current_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 games played so far, 3 wins (60.00 %), 10.00 movs avg\n",
      "10 games played so far, 4 wins (40.00 %), 10.25 movs avg\n",
      "15 games played so far, 6 wins (40.00 %), 8.67 movs avg\n",
      "20 games played so far, 6 wins (30.00 %), 8.67 movs avg\n",
      "25 games played so far, 9 wins (36.00 %), 8.22 movs avg\n",
      "30 games played so far, 10 wins (33.33 %), 8.30 movs avg\n",
      "35 games played so far, 12 wins (34.29 %), 8.67 movs avg\n",
      "40 games played so far, 13 wins (32.50 %), 8.54 movs avg\n",
      "45 games played so far, 14 wins (31.11 %), 8.64 movs avg\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 100\n",
    "\n",
    "wins = 0\n",
    "movs = []\n",
    "for k in range(N_GAMES):\n",
    "    win, mov = simgame(game, agent, logger_simgame)\n",
    "    if win == 1:\n",
    "        wins += 1\n",
    "        movs.append(mov)    \n",
    "    if k>1 and k%5 == 0:\n",
    "        print(\"%d games played so far, %d wins (%.2f %%), %.2f movs avg\" % (k, wins, wins*100.0/k, np.array(movs).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
