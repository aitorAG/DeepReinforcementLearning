{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning using AlphaZero methodology\n",
    "\n",
    "Adapted from https://applied-data.science/blog/how-to-build-your-own-alphazero-ai-using-python-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game will control all the mechanisms to play a game, and agent will emulate a player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to display the board, we need to create a logger. Here we just print the board to the standard output, to get a graps of the current situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogger:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def info(log):\n",
    "        # log is a list of chars resembling the board\n",
    "        print(str(log) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing a first game by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play a game by hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do in the game?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the board looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we introduce a token by the top, it will fall to the bottom. At the bottom, we have the positions 35 to 41, so those are the only actions we can do now.\n",
    "\n",
    "For instance, let's put a token right in the middle, it will fall to the middle position at the bottom, that's position 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two players in this game, 1 and -1. The first player was 1, so the current player should be -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what this player can do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because position 38 is taken, now the player -1 could put a token on top of it, that's it, position 31. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who's the next player?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the game going?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the count of games won by each one of the players. Let's make player -1 win the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second element of the tuple is the value. The value 0 means that nothing has happened yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If player 1 moves to position 37, then player 1 will win. But player 1 is dumb, so the next moves are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the score of the game, we have to check who is the current player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then get the first value of these tuple. The winner of the game is the multiplication of both values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep playing. We need to clear the board to keep playing, because the game goal is to be the first to make a 4-connect. Once that's done, newer 4-connect will not contribute towards the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<game.GameState at 0xb334f1240>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0xb334f1860>, 0, 0, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(38)\n",
    "game.step(31)\n",
    "game.step(35)\n",
    "game.step(24)\n",
    "game.step(36)\n",
    "game.step(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now player 1 has learnt, and will do the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the winner is 1\n"
     ]
    }
   ],
   "source": [
    "print(\"And the winner is %d\" % (game.currentPlayer*game.gameState.score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect that a game has finished, we can monitor the score, or the value returned by each step. When it is different to 0, that means that there has been a winning move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game with an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network using the results of our games, we need to use an agent. The agent needs to use an untrained neural network as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network, we can use any Keras model. Here, we use a function from the game, that needs some configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import Residual_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_CONST=0.0001\n",
    "LEARNING_RATE=0.1\n",
    "\n",
    "HIDDEN_CNN_LAYERS = [\n",
    "\t{'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 3   # number of simulations the agent will attempt to search for the best next movement\n",
    "CPUCT = 1  # constant controlling the level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a blank state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b0f1639e9880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'game' is not defined"
     ]
    }
   ],
   "source": [
    "state = game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5e296be422d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agent will decide what to do next (using 1 for a deterministic move):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the positions in the board, `next_action` is the position with the maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a vector with the probability of all the positions in the board. For instance, we can check that all positions with prob > 0 are in fact allowed actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `act` method, the second argument should be 0 for a deterministic movement, and 1 for a random movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is the turn of the second player (who plays randomly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep playing with this agent, that will try to find the best movements for the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: a learning agent against a random player\n",
    "\n",
    "Now that you know how to run a learning agent in a game, write a function that given an agent returns the outcome of the game.\n",
    "\n",
    "Don't worry about keeping the memory of the positions. We just want the final outcome of the game, from the learning agent point of view: WIN, DRAW or LOSS.\n",
    "\n",
    "The game will be randomly started either by the random player or the neural network.\n",
    "\n",
    "We will later use this function to run several simulations.\n",
    "\n",
    "Use this logger to keep track of:\n",
    "* each new action suggested by the agent (both for the NN and for the random player)\n",
    "* value after each movement\n",
    "* a render of the board (you can use state.render(logger))\n",
    "* if the movement is done by the NN, the values of the MonteCarlo tree search, and the NN network\n",
    "* a big WARNING if the agent suggest a movement that is not allowed by the state of the board\n",
    "\n",
    "The function will return a tuple, with the result of the game, and the number of movements of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_logger\n",
    "\n",
    "logger_simgame = setup_logger('logger_simgame', 'logs/logger_simgame.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student version cell\n",
    "def simgame(game, agent, logger):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @return a tuple with the result of the game and the number of movements of the NN\n",
    "    \"\"\"\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    logger.info(\"NEW GAME\")\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    \n",
    "    state = game.reset()\n",
    "    \n",
    "    # 0 -> the neural network starts\n",
    "    # 1 -> the random player starts\n",
    "    who_starts = random.choice([0,1])\n",
    "    \n",
    "    # Tau is the parameter that controls the act method, 0 is random, 1 is neural network\n",
    "    if who_starts == 0:\n",
    "        tau = 0  # NN starts\n",
    "        logger.info(\"Game started by neural network. NN will be the X\")\n",
    "        nn_symbol, rnd_symbol = \"X\", \"O\"\n",
    "    else:\n",
    "        tau = 1  # Random player starts\n",
    "        logger.info(\"Game started by random player. NN will be the O\")\n",
    "        nn_symbol, rnd_symbol = \"O\", \"X\"\n",
    "        \n",
    "    game_is_ended = False\n",
    "    winner = -2  # we init with an impossible value\n",
    "    nn_movements = 0\n",
    "\n",
    "    while not game_is_ended:\n",
    "\n",
    "        # *** YOUR CODE SHOULD DECIDE THE NEXT ACTION ***\n",
    "        next_action = None\n",
    "\n",
    "        # *** YOU NEED TO UPDATE nn_movements ONLY WHEN THE NEURAL NETWORK PLAYS ***\n",
    "        \n",
    "        # *** YOU SHOULD LOG MORE INFO, FOR INSTANCE, THE STATE OF THE BOARD ***\n",
    "        \n",
    "        if tau == 0:\n",
    "            logger.info(\"NN (%s) played, moved to %d\" % (nn_symbol, next_action))\n",
    "            tau = 1            \n",
    "        else:\n",
    "            tau = 0\n",
    "            logger.info(\"Random (%s) played, moved to %d\" % (rnd_symbol, next_action))\n",
    "\n",
    "        logger.info(\"Game score: %d     MCTS: %.4f          NN: %.4f\" % (score, MCTS_value, NN_value))\n",
    "        if game_is_ended:\n",
    "            # *** WHO HAS WON? WRITE YOUR CODE HERE ***\n",
    "            winner = None\n",
    "            # If random started, then the result of the game is the opposite\n",
    "            if who_starts == 1:\n",
    "                winner = winner*(-1)\n",
    "            if winner == 1:\n",
    "                logger.info(\" **** The NN has WON! :D ****\")\n",
    "            elif winner == 0:\n",
    "                logger.info(\" **** It is a DRAW :S ****\")\n",
    "            else:\n",
    "                logger.info(\" **** The NN has LOST :'( ****\")\n",
    "\n",
    "    return winner, nn_movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the agent learnt?\n",
    "\n",
    "Let's try several times, and plot some stats about the number of wins, and the distribution of the number of movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 10   # number of simulations of movements the agent will attempt to search for the best next movement\n",
    "CPUCT = 1  # constant controlling the level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_CONST=0.0001\n",
    "LEARNING_RATE=0.1\n",
    "\n",
    "HIDDEN_CNN_LAYERS = [\n",
    "\t{'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t]\n",
    "\n",
    "# Create a game and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play N times, and keep track of the winning average of our agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Learning from this experience\n",
    "\n",
    "So far, we are not learning from this experience. We are just playing with a neural network that is not trained.\n",
    "\n",
    "We can add the movements to a _memory_ and record the outcome of the game too, and then train the neural network with this experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE=30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Memory object has two kind of memories:\n",
    "\n",
    "* Short term, with the set of movements of a game\n",
    "* Long term, with the full games and their outcomes. This long term memory is used to re-train the agent and gain experience in the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Memory.commit_stmemory of <memory.Memory object at 0xb31480208>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prepares the memory for a new game\n",
    "memory.clear_stmemory()\n",
    "# This adds a movement to the memory\n",
    "memory.commit_stmemory\n",
    "# This adds a game to the long term (training) memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simgame(game, agent, logger, memory = None):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @param memory a Memory object to record all the movements and outcome of the game\n",
    "    @return a tuple with the result of the game, the number of movements of the NN and the updated memory\n",
    "    \"\"\"\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    logger.info(\"NEW GAME\")\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    \n",
    "    state = game.reset()\n",
    "    \n",
    "    # 0 -> the neural network starts\n",
    "    # 1 -> the random player starts\n",
    "    who_starts = random.choice([0,1])\n",
    "    \n",
    "    # Tau is the parameter that controls the act method, 0 is random, 1 is neural network\n",
    "    if who_starts == 0:\n",
    "        tau = 0  # NN starts\n",
    "        logger.info(\"Game started by neural network. NN will be the X\")\n",
    "        nn_symbol, rnd_symbol = \"X\", \"O\"\n",
    "    else:\n",
    "        tau = 1  # Random player starts\n",
    "        logger.info(\"Game started by random player. NN will be the O\")\n",
    "        nn_symbol, rnd_symbol = \"O\", \"X\"\n",
    "        \n",
    "    game_is_ended = False\n",
    "    winner = -2  # we init with an impossible value\n",
    "    nn_movements = 0\n",
    "\n",
    "    while not game_is_ended:\n",
    "\n",
    "        # *** YOUR CODE SHOULD DECIDE THE NEXT ACTION ***\n",
    "        next_action = None\n",
    "\n",
    "        # *** YOU NEED TO UPDATE nn_movements ONLY WHEN THE NEURAL NETWORK PLAYS ***\n",
    "        \n",
    "        # *** YOU SHOULD LOG MORE INFO, FOR INSTANCE, THE STATE OF THE BOARD ***\n",
    "        \n",
    "        # *** HOW SHOULD UPDATE THE MEMORY OBJECT?\n",
    "        \n",
    "        if tau == 0:\n",
    "            logger.info(\"NN (%s) played, moved to %d\" % (nn_symbol, next_action))\n",
    "            tau = 1            \n",
    "        else:\n",
    "            tau = 0\n",
    "            logger.info(\"Random (%s) played, moved to %d\" % (rnd_symbol, next_action))\n",
    "\n",
    "        logger.info(\"Game score: %d     MCTS: %.4f          NN: %.4f\" % (score, MCTS_value, NN_value))\n",
    "        if game_is_ended:\n",
    "            # *** WHO HAS WON? WRITE YOUR CODE HERE ***\n",
    "            winner = None\n",
    "            # If random started, then the result of the game is the opposite\n",
    "            if who_starts == 1:\n",
    "                winner = winner*(-1)\n",
    "            if winner == 1:\n",
    "                logger.info(\" **** The NN has WON! :D ****\")\n",
    "            elif winner == 0:\n",
    "                logger.info(\" **** It is a DRAW :S ****\")\n",
    "            else:\n",
    "                logger.info(\" **** The NN has LOST :'( ****\")\n",
    "\n",
    "    return winner, nn_movements, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 games played so far, 2 wins (40.00 %), 6.00 movs avg\n",
      "10 games played so far, 4 wins (40.00 %), 6.25 movs avg\n"
     ]
    }
   ],
   "source": [
    "# Play N times, and keep track of the winning average of our agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make our agent learn from this experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we learn from this experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play N times, and keep track of the winning average of our agent\n",
    "# Has the agent improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you retrain after every game? (or after every 5-10 games, to save some time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using a custom model\n",
    "\n",
    "The models that the agent trains are Keras models, created following the interface defined in model.Gen_Model\n",
    "\n",
    "Could you change the model and use a different architecture? For instance, a model with RNN that could try to learn from the sequences of movements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import model\n",
    "reload(model)\n",
    "from model import KSchool_Model  # <--- This is your custom model in model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play N times, and keep track of the winning average of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
