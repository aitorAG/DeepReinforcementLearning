{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning using AlphaZero methodology\n",
    "\n",
    "Adapted from https://applied-data.science/blog/how-to-build-your-own-alphazero-ai-using-python-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game will control all the mechanisms to play a game, and agent will emulate a player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to display the board, we need to create a logger. Here we just print the board to the standard output, to get a graps of the current situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogger:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def info(log):\n",
    "        # log is a list of chars resembling the board\n",
    "        print(str(log) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing a first game by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play a game by hand\n",
    "game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do in the game?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 36, 37, 38, 39, 40, 41]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.allowedActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the board looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we introduce a token by the top, it will fall to the bottom. At the bottom, we have the positions 35 to 41, so those are the only actions we can do now.\n",
    "\n",
    "For instance, let's put a token right in the middle, it will fall to the middle position at the bottom, that's position 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(38)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two players in this game, 1 and -1. The first player was 1, so the current player should be -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what this player can do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 35, 36, 37, 39, 40, 41]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.allowedActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because position 38 is taken, now the player -1 could put a token on top of it, that's it, position 31. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(31)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who's the next player?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the game going?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the count of games won by each one of the players. Let's make player -1 win the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 35, 36, 37, 39, 40, 41]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.allowedActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(35)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.step(24)\n",
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0xb334d6a20>, 0, 0, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(36)\n",
    "game.step(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second element of the tuple is the value. The value 0 means that nothing has happened yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If player 1 moves to position 37, then player 1 will win. But player 1 is dumb, so the next moves are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0xb334d6630>, 0, 0, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0xb334d6748>, -1, 1, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', 'X', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the score of the game, we have to check who is the current player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then get the first value of these tuple. The winner of the game is the multiplication of both values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the winner is -1\n"
     ]
    }
   ],
   "source": [
    "print(\"And the winner is %d\" % (game.currentPlayer*game.gameState.score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep playing. We need to clear the board to keep playing, because the game goal is to be the first to make a 4-connect. Once that's done, newer 4-connect will not contribute towards the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<game.GameState at 0xb334f1240>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0xb334f1860>, 0, 0, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(38)\n",
    "game.step(31)\n",
    "game.step(35)\n",
    "game.step(24)\n",
    "game.step(36)\n",
    "game.step(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', '-', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now player 1 has learnt, and will do the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<game.GameState at 0xb334f12e8>, -1, 1, None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.step(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', 'O', '-', '-', '-']\n",
      "\n",
      "['X', 'X', 'X', 'X', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.gameState.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.gameState.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.currentPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the winner is 1\n"
     ]
    }
   ],
   "source": [
    "print(\"And the winner is %d\" % (game.currentPlayer*game.gameState.score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect that a game has finished, we can monitor the score, or the value returned by each step. When it is different to 0, that means that there has been a winning move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game with an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network using the results of our games, we need to use an agent. The agent needs to use an untrained neural network as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network, we can use any Keras model. Here, we use a function from the game, that needs some configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Residual_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_CONST=0.0001\n",
    "LEARNING_RATE=0.1\n",
    "\n",
    "HIDDEN_CNN_LAYERS = [\n",
    "\t{'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ihr/github/DeepReinforcementLearning/loss.py:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_NN = Residual_CNN(REG_CONST, LEARNING_RATE, (2,) + game.grid_shape, game.action_size, HIDDEN_CNN_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 3   # number of simulations the agent will attempt to search for the best next movement\n",
    "CPUCT = 1  # constant controlling the level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"Lee Sedol del Conecta4\", game.state_size, game.action_size, NUM_OF_SIMULATIONS, CPUCT, current_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a blank state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agent will decide what to do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_action, probs, MCTS_value, NN_value = agent.act(state, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the positions in the board, `next_action` is the position with the maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0.5, 0. , 0. ,\n",
       "       0. , 0. , 0. ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a vector with the probability of all the positions in the board. For instance, we can check that all positions with prob > 0 are in fact allowed actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 36, 37, 38, 39, 40, 41]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.allowedActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35],\n",
       "       [39]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(probs > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, value, _, _ = game.step(next_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `act` method, the second argument should be 0 for a deterministic movement, and 1 for a random movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is the turn of the second player (who plays randomly)\n",
    "next_action, probs, _, _ = agent.act(state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "       0.33333333, 0.33333333, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 35, 36, 37, 38, 40, 41]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.allowedActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, value, _, _ = game.step(next_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', 'X', '-', '-']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep playing with this agent, that will try to find the best movements for the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', 'X', '-', 'X']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_action, probs, _, _ = agent.act(state, 1)\n",
    "state, value, _, _ = game.step(next_action)\n",
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', 'X', '-', 'X']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_action, probs, _, _ = agent.act(state, 0)\n",
    "state, value, _, _ = game.step(next_action)\n",
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['-', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['X', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', '-', '-', '-']\n",
      "\n",
      "['O', '-', '-', '-', 'X', '-', 'X']\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_action, probs, _, _ = agent.act(state, 1)\n",
    "state, value, _, _ = game.step(next_action)\n",
    "state.render(mylogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: a learning agent against a random player\n",
    "\n",
    "Now that you know how to run a learning agent in a game, write a function that given an agent returns the outcome of the game.\n",
    "\n",
    "Don't worry about keeping the memory of the positions. We just want the final outcome of the game, from the learning agent point of view: WIN, DRAW or LOSS.\n",
    "\n",
    "The game will be randomly started either by the random player or the neural network.\n",
    "\n",
    "We will later use this function to run several simulations.\n",
    "\n",
    "Use this logger to keep track of:\n",
    "* each new action suggested by the agent (both for the NN and for the random player)\n",
    "* value after each movement\n",
    "* a render of the board (you can use state.render(logger))\n",
    "* if the movement is done by the NN, the values of the MonteCarlo tree search, and the NN network\n",
    "* a big WARNING if the agent suggest a movement that is not allowed by the state of the board\n",
    "\n",
    "The function will return a tuple, with the result of the game, and the number of movements of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_logger\n",
    "\n",
    "logger_simgame = setup_logger('logger_simgame', 'logs/logger_simgame.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student version cell\n",
    "def simgame(game, agent, logger):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @return a tuple with the result of the game and the number of movements of the NN\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simgame(game, agent, logger):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @return a tuple with the result of the game and the number of movements of the NN\n",
    "    \"\"\"\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    logger.info(\"NEW GAME\")\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    \n",
    "    state = game.reset()\n",
    "    \n",
    "    # 0 -> the neural network starts\n",
    "    # 1 -> the random player starts\n",
    "    who_starts = random.choice([0,1])\n",
    "    \n",
    "    # Tau is the parameter that controls the act method, 0 is random, 1 is neural network\n",
    "    if who_starts == 0:\n",
    "        tau = 0  # NN starts\n",
    "        logger.info(\"Game started by neural network. NN will be the X\")\n",
    "        nn_symbol, rnd_symbol = \"X\", \"O\"\n",
    "    else:\n",
    "        tau = 1  # Random player starts\n",
    "        logger.info(\"Game started by random player. NN will be the O\")\n",
    "        nn_symbol, rnd_symbol = \"O\", \"X\"\n",
    "        \n",
    "    game_is_ended = False\n",
    "    winner = -2  # we init with an impossible value\n",
    "    nn_movements = 0\n",
    "    while not game_is_ended:\n",
    "        next_action, _, MCTS_value, NN_value = agent.act(state, tau)\n",
    "        state, score, _, _ = game.step(next_action)\n",
    "        state.render(logger)\n",
    "        if tau == 0:\n",
    "            logger.info(\"NN (%s) played, moved to %d\" % (nn_symbol, next_action))\n",
    "            tau = 1\n",
    "            nn_movements += 1\n",
    "        else:\n",
    "            tau = 0\n",
    "            logger.info(\"Random (%s) played, moved to %d\" % (rnd_symbol, next_action))\n",
    "            \n",
    "        logger.info(\"Game score: %d     MCTS: %.4f          NN: %.4f\" % (score, MCTS_value, NN_value))\n",
    "        if state.isEndGame != 0:\n",
    "            game_is_ended = True\n",
    "            winner = game.currentPlayer*score\n",
    "            # If random started, then the result of the game is the opposite\n",
    "            if who_starts == 1:\n",
    "                winner = winner*(-1)\n",
    "            if winner == 1:\n",
    "                logger.info(\" **** The NN has WON! :D ****\")\n",
    "            elif winner == 0:\n",
    "                logger.info(\" **** It is a DRAW :S ****\")\n",
    "            else:\n",
    "                logger.info(\" **** The NN has LOST :'( ****\")\n",
    "                \n",
    "    return winner, nn_movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the agent learnt?\n",
    "\n",
    "Let's try several times, and plot some stats about the number of wins, and the distribution of the number of movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_SIMULATIONS = 10   # number of simulations of movements the agent will attempt to search for the best next movement\n",
    "CPUCT = 1  # constant controlling the level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dfac48b1923c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \t]\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcurrent_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidual_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREG_CONST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_CNN_LAYERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lee Sedol del Conecta4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_OF_SIMULATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCPUCT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_NN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Game' is not defined"
     ]
    }
   ],
   "source": [
    "REG_CONST=0.0001\n",
    "LEARNING_RATE=0.1\n",
    "\n",
    "HIDDEN_CNN_LAYERS = [\n",
    "\t{'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t , {'filters':75, 'kernel_size': (4,4)}\n",
    "\t]\n",
    "\n",
    "game = Game()\n",
    "current_NN = Residual_CNN(REG_CONST, LEARNING_RATE, (2,) + game.grid_shape, game.action_size, HIDDEN_CNN_LAYERS)\n",
    "agent = Agent(\"Lee Sedol del Conecta4\", game.state_size, game.action_size, NUM_OF_SIMULATIONS, CPUCT, current_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 games played so far, 3 wins (60.00 %), 6.33 movs avg\n",
      "10 games played so far, 8 wins (80.00 %), 6.25 movs avg\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 10\n",
    "\n",
    "wins = 0\n",
    "movs = []\n",
    "for k in range(1, N_GAMES+1):\n",
    "    win, mov = simgame(game, agent, logger_simgame)\n",
    "    if win == 1:\n",
    "        wins += 1\n",
    "        movs.append(mov)    \n",
    "    if k%5 == 0:\n",
    "        print(\"%d games played so far, %d wins (%.2f %%), %.2f movs avg\" % (k, wins, wins*100.0/k, np.array(movs).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from this experience\n",
    "\n",
    "So far, we are not learning from this experience. We are just playing with a neural network that is not trained.\n",
    "\n",
    "We can add the movements to a _memory_ and record the outcome of the game too, and then train the neural network with this experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE=30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Memory object has two kind of memories:\n",
    "\n",
    "* Short term, with the set of movements of a game\n",
    "* Long term, with the full games and their outcomes. This long term memory is used to re-train the agent and gain experience in the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Memory.commit_stmemory of <memory.Memory object at 0xb34accc18>>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prepares the memory for a new game\n",
    "memory.clear_stmemory()\n",
    "# This adds a movement to the memory\n",
    "memory.commit_stmemory\n",
    "# This adds a game to the long term (training) memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simgame(game, agent, logger, memory = None):\n",
    "    \"\"\"Sim a game and return the outcome of the game. \n",
    "    \n",
    "    @param game a Game that will be played by the agent. This game will be reset\n",
    "    @param agent an Agent with an associated neural network\n",
    "    @param logger a logger to keep track of the internal statuses\n",
    "    @param memory a Memory object to record all the movements and outcome of the game\n",
    "    @return a tuple with the result of the game, the number of movements of the NN and the updated memory\n",
    "    \"\"\"\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    logger.info(\"NEW GAME\")\n",
    "    logger.info(\"---------------------------------------\")\n",
    "    \n",
    "    state = game.reset()\n",
    "    if memory:\n",
    "        memory.clear_stmemory()\n",
    "        \n",
    "    # 0 -> the neural network starts\n",
    "    # 1 -> the random player starts\n",
    "    who_starts = random.choice([0,1])\n",
    "    \n",
    "    # Tau is the parameter that controls the act method, 0 is random, 1 is neural network\n",
    "    if who_starts == 0:\n",
    "        tau = 0  # NN starts\n",
    "        logger.info(\"Game started by neural network. NN will be the X\")\n",
    "        nn_symbol, rnd_symbol = \"X\", \"O\"\n",
    "    else:\n",
    "        tau = 1  # Random player starts\n",
    "        logger.info(\"Game started by random player. NN will be the O\")\n",
    "        nn_symbol, rnd_symbol = \"O\", \"X\"\n",
    "        \n",
    "    game_is_ended = False\n",
    "    winner = -2  # we init with an impossible value\n",
    "    nn_movements = 0\n",
    "    while not game_is_ended:\n",
    "        next_action, probs, MCTS_value, NN_value = agent.act(state, tau)\n",
    "        state, score, _, _ = game.step(next_action)\n",
    "        state.render(logger)\n",
    "        memory.commit_stmemory(game.identities, state, probs)\n",
    "        if tau == 0:\n",
    "            logger.info(\"NN (%s) played, moved to %d\" % (nn_symbol, next_action))\n",
    "            tau = 1\n",
    "            nn_movements += 1\n",
    "        else:\n",
    "            tau = 0\n",
    "            logger.info(\"Random (%s) played, moved to %d\" % (rnd_symbol, next_action))\n",
    "            \n",
    "        logger.info(\"Game score: %d     MCTS: %.4f          NN: %.4f\" % (score, MCTS_value, NN_value))\n",
    "        if state.isEndGame != 0:\n",
    "            game_is_ended = True\n",
    "            winner = game.currentPlayer*score\n",
    "            # If random started, then the result of the game is the opposite\n",
    "            if who_starts == 1:\n",
    "                winner = winner*(-1)\n",
    "            if winner == 1:\n",
    "                logger.info(\" **** The NN has WON! :D ****\")\n",
    "            elif winner == 0:\n",
    "                logger.info(\" **** It is a DRAW :S ****\")\n",
    "            else:\n",
    "                logger.info(\" **** The NN has LOST :'( ****\")\n",
    "                \n",
    "                \n",
    "    # Commit long term memory\n",
    "    if memory != None:\n",
    "        if who_starts == 0:\n",
    "            multiplier = 1\n",
    "        else:\n",
    "            multiplier = -1\n",
    "            \n",
    "        #### If the game is finished, assign the values correctly to the game moves\n",
    "        for move in memory.stmemory:\n",
    "            if move['playerTurn'] == state.playerTurn:\n",
    "                move['value'] = multiplier*winner\n",
    "            else:\n",
    "                move['value'] = -multiplier*winner\n",
    "        memory.commit_ltmemory()\n",
    "\n",
    "    return winner, nn_movements, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 games played so far, 2 wins (40.00 %), 6.00 movs avg\n",
      "10 games played so far, 4 wins (40.00 %), 6.25 movs avg\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 10\n",
    "\n",
    "memory = Memory(MEMORY_SIZE)\n",
    "\n",
    "wins = 0\n",
    "movs = []\n",
    "for k in range(1, N_GAMES+1):\n",
    "    win, mov, memory = simgame(game, agent, logger_simgame, memory)\n",
    "    if win == 1:\n",
    "        wins += 1\n",
    "        movs.append(mov)    \n",
    "    if k%5 == 0:\n",
    "        print(\"%d games played so far, %d wins (%.2f %%), %.2f movs avg\" % (k, wins, wins*100.0/k, np.array(movs).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make our agent learn from this experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "256/256 [==============================] - 2s 9ms/step - loss: nan - value_head_loss: nan - policy_head_loss: nan    \n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 471us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 490us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 712us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 554us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 531us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 515us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 669us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 548us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 499us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n"
     ]
    }
   ],
   "source": [
    "agent.replay(memory.ltmemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 games played so far, 3 wins (60.00 %), 4.33 movs avg\n",
      "10 games played so far, 8 wins (80.00 %), 6.50 movs avg\n",
      "15 games played so far, 11 wins (73.33 %), 7.55 movs avg\n",
      "20 games played so far, 14 wins (70.00 %), 7.93 movs avg\n",
      "25 games played so far, 19 wins (76.00 %), 8.26 movs avg\n",
      "30 games played so far, 21 wins (70.00 %), 8.00 movs avg\n",
      "35 games played so far, 26 wins (74.29 %), 8.46 movs avg\n",
      "40 games played so far, 27 wins (67.50 %), 8.70 movs avg\n",
      "45 games played so far, 29 wins (64.44 %), 8.86 movs avg\n",
      "50 games played so far, 30 wins (60.00 %), 8.93 movs avg\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 50\n",
    "wins = 0\n",
    "movs = []\n",
    "for k in range(1, N_GAMES+1):\n",
    "    win, mov, memory = simgame(game, agent, logger_simgame, memory)\n",
    "    if win == 1:\n",
    "        wins += 1\n",
    "        movs.append(mov)    \n",
    "    if k%5 == 0:\n",
    "        print(\"%d games played so far, %d wins (%.2f %%), %.2f movs avg\" % (k, wins, wins*100.0/k, np.array(movs).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simgame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b188bc8fcbf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_GAMES\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimgame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_simgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwin\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mwins\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simgame' is not defined"
     ]
    }
   ],
   "source": [
    "N_GAMES = 50\n",
    "wins = 0\n",
    "movs = []\n",
    "for k in range(1, N_GAMES+1):\n",
    "    win, mov, memory = simgame(game, agent, logger_simgame, memory)\n",
    "    if win == 1:\n",
    "        wins += 1\n",
    "        movs.append(mov)    \n",
    "    if k%5 == 0:\n",
    "        print(\"%d games played so far, %d wins (%.2f %%), %.2f movs avg\" % (k, wins, wins*100.0/k, np.array(movs).mean()))\n",
    "        print(\"Retraining...\")\n",
    "        agent.replay(memory.ltmemory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a custom model\n",
    "\n",
    "The models that the agent trains are Keras models, created following the interface defined in model.Gen_Model\n",
    "\n",
    "Could you change the model and use a different architecture? For instance, a model with RNN that could try to learn from the sequences of movements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import model\n",
    "reload(model)\n",
    "from model import KSchool_Model  # <--- This is your custom model in model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_NN = KSchool_Model(REG_CONST, LEARNING_RATE, (2,) + game.grid_shape, game.action_size)\n",
    "agent = Agent(\"Lee Sedol del Conecta4\", game.state_size, game.action_size, NUM_OF_SIMULATIONS, CPUCT, current_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 games played so far, 3 wins (60.00 %), 8.67 movs avg\n",
      "Retraining...\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 2s 11ms/step - loss: nan - value_head_loss: nan - policy_head_loss: nan                                         \n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 526us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 497us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 514us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 567us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 534us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 533us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 510us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 507us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 730us/step - loss: nan - value_head_loss: nan - policy_head_loss: nan\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 50\n",
    "wins = 0\n",
    "movs = []\n",
    "for k in range(1, N_GAMES+1):\n",
    "    win, mov, memory = simgame(game, agent, logger_simgame, memory)\n",
    "    if win == 1:\n",
    "        wins += 1\n",
    "        movs.append(mov)    \n",
    "    if k%5 == 0:\n",
    "        print(\"%d games played so far, %d wins (%.2f %%), %.2f movs avg\" % (k, wins, wins*100.0/k, np.array(movs).mean()))\n",
    "        print(\"Retraining...\")\n",
    "        agent.replay(memory.ltmemory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
